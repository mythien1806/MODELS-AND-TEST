---
title: "Multiple Regression"
author: "Nhat My Thien Nguyen"
date: "4/3/2021"
output: html_document
---

```{r setup, include=TRUE, warning=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(kableExtra)
library(readr)
library(readxl)
library(lmtest)
library(stargazer)
library(car)
library(broom)
```

# Problem 1

### (i) Use OLS to estimate a model relating colGPA to hsGPA, ACT, skipped, and PC. Obtain the OLS residuals e_i


```{r}
colGPA<-read_excel("colGPA_HW2.xls")
OLS<-lm(colGPA~hsGPA+ACT+skipped+pc,data=colGPA)
summary(OLS)

residual<-residuals(OLS)
yhat<-fitted(OLS)
plot(yhat,residual,xlab="fitted values",ylab="residuals",col=4,pch=20)
```

### (ii) Compute the special case of the White test for heteroskedasticity

```{r}
#H0 : the model is homoscedastic
whitetest<-bptest(OLS,~fitted(OLS)+I(fitted(OLS)^2))
whitetest
```

Given 95% confident level, this test results implies the model is heteroscedastic (reject Null hypothesis) since p-value 0.043<0.05.

### (iii) Now regress  e_i^2 on the fitted values of colGPA and colGPA2 and save the fitted  e_i^2  as  h_i^   

```{r}
e_sq<-residual^2
OLS2<-lm(e_sq~yhat+I(yhat^2))
summary(OLS2)
hi<-fitted(OLS2)
```

### (iv)Verify that the fitted values from part (iii) are all strictly positive. Then, obtain the weighted least squares estimates using weights 1/hi . Compare the weighted least squares estimates for the effect of skipping lectures and the effect of PC ownership with the corresponding OLS estimates. What about their statistical significance?


```{r}
min_hi<-min(hi)
min_hi
```
Since minimum of fitted values (hi) is 0.0192, we can	conclude that the fitted values are all strictly positive.

```{r}
colGPA_w <-1/hi
colGPA_wls<-lm(colGPA~hsGPA+ACT+skipped+pc,weights=colGPA_w,data=colGPA)

stargazer(OLS,colGPA_wls,header=F,
          title="OLS vs WLS estimates for the college GPA",
          type='text',
          omit.table.layout="n",
          star.cutoffs=NA,
          digits=3,
          intercept.bottom=FALSE,
          column.labels=c("OLS", "WLS"),
          dep.var.labels.include = FALSE,
          model.numbers = FALSE,
          dep.var.caption="Dependent variable: colGPA",
          model.names=FALSE,
          star.char=NULL)
```

We can see the R^2 and adjusted R^2 of WLS model are greater than those of OLS model.

Coefficient of ACT is the same between 2 model, and coefficients of skipped and pc between 2 model are very small different.

### (v) In the WLS estimation from part (iv), obtain heteroskedasticity-robust standard errors. In other words, allow for the fact that the variance function estimated in part (ii) might be misspecified. Do the standard errors change much from part (iv)?


```{r}
cov1<-hccm(colGPA_wls,type="hc1")
colGPA_hc1<-coeftest(colGPA_wls,vcov.=cov1)

stargazer(colGPA_wls,colGPA_hc1,header=F,
          title="WLS vs WLS with RSE",
          type='text',
          omit.table.layout="n",
          star.cutoffs=NA,
          digits=3,
          intercept.bottom=FALSE,
          column.labels=c("WLS", "WLS with RSE"),
          dep.var.labels.include = FALSE,
          model.numbers = FALSE,
          dep.var.caption="Dependent variable: colGPA",
          model.names=FALSE,
          star.char=NULL
          )

```

The result shows the standard errors of WLS with RSE do not change much (very small greater) from original WLS model.

# Problem 2

A real estate association in a suburban community would like to study the relationship between the size of a single-family house (as measured by the square footage) and the selling price of the house (in thousands of dollars). 

```{r}
houseprice<-read_csv("HousePrice.csv")
```

### (i) Undertake appropriate basic data analytics to motivate the regression model above (check for all variables you need to include and if polynomial terms are necessary. 

```{r}
correlation.matrix<-cor(houseprice)
kable(correlation.matrix)

mod1<-lm(Price~Size,data=houseprice)
mod2<-lm(Price~Size+West,data=houseprice)
mod3<-lm(Price~Size+West+FencedYard,data=houseprice)

r1<-as.numeric(glance(mod1))
r2<-as.numeric(glance(mod2))
r3<-as.numeric(glance(mod3))

tab<-data.frame(rbind(r1,r2,r3))[,c(1,2,8,9)]
row.names(tab)<-c("Price~Size","Price~Size+West","Price~Size+West+FencedYard")
kable(tab,caption="Model Comparison",digits=3,
      col.names=c("RSq","AdjRsq","AIC","BIC"))%>%
  kable_styling(full_width = T)
```

It is noticeable that two criteria indicate the second model is the best fit.

Now we do the resettest for the second model
H0 : "No higher-order polynomial terms are necessary"

```{r}
resettest(mod2,power=2,type="regressor")
```

Given confident level at 95%, since p-value=0.1068 > 0.05, we do not reject H0, which implies there is no need higher-order polynomial terms for this model.

### (ii) Once you have decided on the best specification, use that regression model of the Price on the relevant house characteristics and interpret all regression coefficients.

```{r}
summary(mod2)
```

The coefficient of Size = 8.12, which means 100 square feet increase in Size, the price of house will increase 8.12 thousands dollar respectively.
The coefficient of West is 59.82, which means a house of the same square feet will cost extra 59.82 thousands dollar more when located in the West. 

### (iii) Undertake the postestimation analytics and tests to determine the validity of the regression model and the individual variables. Is there any heteroscedasticity in the model you used in part (b)?  If needed use the White’s corrected Standard errors to interpret the results of the model.

```{r}
res<-residuals(mod2)
yhat<-fitted(mod2)
par(mfrow=c(1,2))
plot(houseprice$Size,res,xlab="Size",ylab="residuals",col=4)
plot(yhat,res,xlab="fitted values",ylab="residuals",col=4)
```

Now use the White's Test to test for homoscedascity.
H0 : The model is Homoscedastic

```{r}
bptest(mod2,~fitted(mod2)+I(fitted(mod2)^2))
```

Since p-value = 0.3606 is quite big, do not reject H0, which means the residuals of model is homoscedastic. Besides, the residuals plots also show that there is no heteroscedasticity in the model.

### (iv) Run another multiple regression equation that predicts the selling price, based on the size, X1, the neighborhood or the side of the city (West), X2, and their interaction term, X1 × X2.   

```{r}
mod4<-lm(Price~Size*West,data=houseprice)
summary(mod4)
```

### (v) Is there any heteroscedasticity in the model you used in part (d)?  If needed use the White’s corrected Standard errors this model.


```{r}
res1<-residuals(mod4)
yhat1<-fitted(mod4)
par(mfrow=c(1,2))
plot(houseprice$Size,res1,xlab="Size",ylab="residuals",col=4)
plot(yhat1,res1,xlab="fitted values",ylab="residuals",col=4)
```

Now use the White's Test to test for homoscedascity.
H0 : The model 4 is Homoscedastic

```{r}
bptest(mod4,~fitted(mod4)+I(fitted(mod4)^2))
```

Since p-value = 0.1554, at 95% confident level, do not reject H0, which means the residuals of model 4 is homoscedastic. Besides, the residuals plots also show that there is no heteroscedasticity in the interations model.

### (vi) Of the two models, use the specification that would best explain the house prices, to predict the price of a house in the West side with 2500 square feet, including the confidence interval of the predicted mean house price and also give the confidence and prediction intervals.

```{r}
r4<-as.numeric(glance(mod4))
tab1<-data.frame(rbind(r2,r4))[,c(1,2,8,9)]
row.names(tab1)<-c("Price~Size+West","Price~Size*West")
kable(tab1,caption="Model Comparison 2",digits=3,
      col.names=c("RSq","AdjRsq","AIC","BIC"))%>%
  kable_styling(full_width = T)
```

The two criteria indicate the interaction model is better to predict the Price of House.
Now use this model to predict the price of a house in the West side with 2500 square feet.

```{r}
example<-data.frame(Size=25,West=1)
predict1<-predict(mod4,example,interval="confidence",conf.level=0.95)
predict1
```

```{r}
predict2<-predict(mod4,example,interval="prediction",conf.level=0.95)
predict2
```




